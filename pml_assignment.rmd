### Prediction Assignment Writeup for Practical Machine Learning.

library(caret)
library(knitr)

training<-read.csv("pml-training.csv")
testing<-read.csv("pml-testing.csv")

dim(training)
dim(testing)
summary(training)

###----------------Data Pre-processing--------------------###
set.seed(20)

# delete the variables with nearly zero variance.
zerov<-nearZeroVar(training)
training1<-training[,-zerov]
testing1<-testing[,-zerov]

# delete the variables with missingness (NA) is over 50%.
halfNA<-sapply(training1,function(x) mean(is.na(x)))>0.50
TrainingData<-training1[,halfNA==F]
TestingData<-testing1[,halfNA==F]

# delete the first five variables: X, user_name, raw_timestramp_part_1,raw_timestamp_part_2,cvtd_timestamp.
TrainingData<-TrainingData[,-(1:5)]
TestingData<-TestingData[,-(1:5)]

# so now, "TrainingData" is the training set. "TestingData" is the observation to test.


###---------------------PCA analysis----------------------### 
# because many variables are highly correlated to each other, so we can use PCA to reduce the dimension.
# bue when using PCA, it will decrease the interpretability of the model.
# so we decide not to use PCA method. 

# preproc=preProcess(TrainingData,method='pca',pcaComp=10)
# train.pca=predict(preproc,TrainingData)
# train.pca=cbind(train.pca,classe)

# preproc20=preProcess(TestingData,method='pca',pcaComp=10)
# test.pca=predict(preproc20,TestingData)

# TrainingData=train.pca
# TestingData=test.pca


###----------------------------------------------------------------------------
###------spilt the "TrainingData" into the training set and testing set-----###
#the "TrainingData" observations is set into 60% of "traindata" and 40% of "testdata".
trainindex<-createDataPartition(TrainingData$classe,p=0.6,list=FALSE)
traindata=TrainingData[trainindex,]
testdata=TrainingData[-trainindex,]
dim(traindata)
dim(testdata)
set.seed(1234)


###-------------------------model building---------------------------------###
# we can use three methods to fit the data: 
# (1)Linear discriminant anaalysis (lda),it applies Bayes theorem to classify observations into the most probable class.
# (2)classification tree(rpart), it is implementation of Classification and Regression Trees developed by Breiman et al. 1984.
# (3)random forest (rf), it is constructed on bootstrapped training samples.
# rf method is one of the best performers that are still interpretable, and more robust to correlated variables.

# 5 fold random forest method (rf):
fitControl<-trainControl(method="cv",number=5,verboseIter=F)
rf.fit<-train(classe ~., data=traindata,method="rf",trControl=fitControl)
rf.fit$finalModel

# classification tree method (rpart):
rpart.fit<-train(classe ~., data=traindata,method="rpart")
rpart.fit$finalModel

# linear discriminant analysis (lda):
lda.fit<-train(classe ~., data=traindata,method="lda")
lda.fit$finalModel
 

###------comparing these three models,using confusionMatrix fuction-------###
# it shows the rf method is superior in comparision to other two.

# rf method: confustion matrix(confmat.rf) and overall accuracy (ConfMat.rf$overall[1]).
confmat.rf<-confusionMatrix(classe, predict(rf.fit,newdata=testdata))
ConfMat.rf$overall[1]

# rpart method: confustion matrix(confmat.rpart) and overall accuracy (ConfMat.rpart$overall[1]).
confmat.rpart<-confusionMatrix(classe, predict(rpart.fit,newdata=testdata))
ConfMat.rpart$overall[1]

# lda method: confustion matrix(confmat.lda) and overall accuracy (ConfMat.lda$overall[1]).
confmat.lda<-confusionMatrix(classe, predict(lda.fit,newdata=testdata))
ConfMat.lda$overall[1] 

##### so then i will use the rf method to predict.



###------predict the 20 observation in the TestingData set----------###
preds<-as.character(predict(rf.fit,TestingData))
print(preds)


###------------to write the predictions to files-------------------###
pml_write=function(x){
  n=lenght(x)
  for (i in 1:n){
      filename=paste0("problem_id_",i,".txt")
      write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
   }
 }

pml_write(preds)
